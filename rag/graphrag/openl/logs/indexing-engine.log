10:49:13,669 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
10:49:20,405 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:49:22,702 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/embeddings "HTTP/1.1 200 OK"
10:49:22,706 graphrag.cli.index INFO Starting pipeline run. dry_run=False
10:49:22,707 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "https://api.fe8.cn/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "https://api.fe8.cn/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
10:49:22,709 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\output
10:49:22,710 graphrag.index.input.factory INFO loading input from root_dir=input
10:49:22,710 graphrag.index.input.factory INFO using file storage for input
10:49:22,712 graphrag.storage.file_pipeline_storage INFO search C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\input for files matching .*\.txt$
10:51:03,1 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
10:51:11,170 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:51:13,855 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/embeddings "HTTP/1.1 200 OK"
10:51:13,858 graphrag.cli.index INFO Starting pipeline run. dry_run=False
10:51:13,859 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "https://api.fe8.cn/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "https://api.fe8.cn/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
10:51:13,860 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\output
10:51:13,861 graphrag.index.input.factory INFO loading input from root_dir=input
10:51:13,861 graphrag.index.input.factory INFO using file storage for input
10:51:13,862 graphrag.storage.file_pipeline_storage INFO search C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\input for files matching .*\.txt$
10:52:17,243 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
10:52:20,654 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:23,631 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/embeddings "HTTP/1.1 200 OK"
10:52:23,636 graphrag.cli.index INFO Starting pipeline run. dry_run=False
10:52:23,636 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "gpt-4-turbo-preview",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "https://api.fe8.cn/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "https://api.fe8.cn/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
10:52:23,638 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\output
10:52:23,638 graphrag.index.input.factory INFO loading input from root_dir=input
10:52:23,639 graphrag.index.input.factory INFO using file storage for input
10:52:23,640 graphrag.storage.file_pipeline_storage INFO search C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\input for files matching .*\.txt$
10:52:23,662 graphrag.index.input.util INFO Found 7 InputFileType.text files, loading 7
10:52:23,665 graphrag.index.input.util INFO Total number of unfiltered InputFileType.text rows: 7
10:52:23,668 graphrag.index.run.run_pipeline INFO Final # of rows loaded: 7
10:52:23,711 graphrag.utils.storage INFO reading table from storage: documents.parquet
10:52:23,806 graphrag.utils.storage INFO reading table from storage: documents.parquet
10:52:23,810 graphrag.utils.storage INFO reading table from storage: text_units.parquet
10:52:23,861 graphrag.utils.storage INFO reading table from storage: text_units.parquet
10:52:31,421 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:31,784 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:31,927 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:31,984 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:32,71 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:32,518 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:32,614 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:32,769 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:33,64 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:33,375 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:38,718 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:38,856 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:39,595 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:39,689 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:40,77 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:40,278 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:41,902 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:42,384 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:43,268 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:46,395 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:46,715 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:47,671 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:49,413 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:52:57,998 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:53:03,171 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:53:03,249 graphrag.utils.storage INFO reading table from storage: entities.parquet
10:53:03,264 graphrag.utils.storage INFO reading table from storage: relationships.parquet
10:53:03,357 graphrag.utils.storage INFO reading table from storage: entities.parquet
10:53:03,374 graphrag.utils.storage INFO reading table from storage: relationships.parquet
10:53:03,448 graphrag.utils.storage INFO reading table from storage: text_units.parquet
10:53:03,452 graphrag.utils.storage INFO reading table from storage: entities.parquet
10:53:03,458 graphrag.utils.storage INFO reading table from storage: relationships.parquet
10:53:03,519 graphrag.utils.storage INFO reading table from storage: relationships.parquet
10:53:03,523 graphrag.utils.storage INFO reading table from storage: entities.parquet
10:53:03,526 graphrag.utils.storage INFO reading table from storage: communities.parquet
10:53:03,566 graphrag.index.operations.summarize_communities.graph_context.context_builder INFO Number of nodes at level=0 => 8
10:53:21,329 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:53:23,35 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:53:23,499 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/chat/completions "HTTP/1.1 200 OK"
10:53:23,565 graphrag.utils.storage INFO reading table from storage: documents.parquet
10:53:23,576 graphrag.utils.storage INFO reading table from storage: relationships.parquet
10:53:23,580 graphrag.utils.storage INFO reading table from storage: text_units.parquet
10:53:23,594 graphrag.utils.storage INFO reading table from storage: entities.parquet
10:53:23,597 graphrag.utils.storage INFO reading table from storage: community_reports.parquet
10:53:23,612 graphrag.index.workflows.generate_text_embeddings INFO Creating embeddings
10:53:23,612 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
10:53:23,832 graphrag.index.operations.embed_text.strategies.openai INFO embedding 12 inputs via 12 snippets using 2 batches. max_batch_size=16, max_tokens=8191
10:53:26,784 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/embeddings "HTTP/1.1 200 OK"
10:53:27,134 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/embeddings "HTTP/1.1 200 OK"
10:53:27,475 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
10:53:27,480 graphrag.index.operations.embed_text.strategies.openai INFO embedding 3 inputs via 3 snippets using 1 batches. max_batch_size=16, max_tokens=8191
10:53:27,861 httpx INFO HTTP Request: POST https://api.fe8.cn/v1/embeddings "HTTP/1.1 200 OK"
10:53:27,937 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'prompt': ["# Employer Liability Insurance and its Digital Processing Tools\n\nThe community comprises Employer Liability Insurance, Ping An Enterprise Treasure App, and Workers Compensation Fund. Employer Liability Insurance is a central entity utilizing the Ping An app for managing claims and offering a streamlined process compared to the Workers Compensation Fund.\n\n## Employer Liability Insurance as a central figure\n\nEmployer Liability Insurance stands as the pivotal entity within this community, dealing with claims from employees concerning injuries or illnesses during employment. This insurance forms the backbone for the community's claim processing abilities and is essential in providing employers and employees with trust and safety in the workplace [Data: Entities (52)]. The relationship with digital tools and other funds suggests a sophisticated and well-managed approach [Data: Relationships (41, 43, +more)].\n\n## Integration with Ping An Enterprise Treasure App\n\nThe integration of Employer Liability Insurance with the Ping An Enterprise Treasure App showcases the trend towards digitization in handling claims. This app facilitates online claims and settlements, significantly simplifying and speeding up the process for employers which could otherwise be more tedious and time-consuming [Data: Entities (54); Relationships (41)]. This modern approach can not only speed up operations but also reduce errors associated with manual processing.\n\n## Comparison with Workers Compensation Fund\n\nWhile both Employer Liability Insurance and Workers Compensation Fund offer solutions for claims arising from workplace incidents, the relationship between them highlights a streamlined process offered by Employer Liability Insurance. Employer Liability Insurance simplifies the claiming process more effectively compared to the traditional methods used by Workers Compensation Fund [Data: Entities (55); Relationships (43)]. This could potentially make Employer Liability Insurance a more preferred choice for companies looking for efficient claiming processes.", "# Employer's and Worker's Insurance Coverage Community\n\nThis community focuses on insurance policies related to workplace incidents, prominently featuring Employer's Liability Insurance, Work Injury Insurance, and Group Accidental Injury Insurance. The structure of this community illustrates the interconnected coverage and legal obligations bestowed upon employers towards their employees, aiming to enhance employee benefits and mitigate labor disputes.\n\n## Utility of Employer's Liability Insurance\n\nEmployer's Liability Insurance is a key component in the insurance landscape, offering comprehensive coverage for liabilities including employee accidents or occupational diseases. The specific packages available — A, B, and 94 versions — differ in terms of the extent of coverage and compensations outlined, suggesting an adaptable solution for varying employer needs [Data: Entities (49); Relationships (37)]. The direct relation of this insurance to legal compliance underlines its importance in reducing potential labor disputes and enhancing employee security [Data: Relationships (37)].\n\n## Comparison between Employer's Liability and Work Injury Insurance\n\nA comparison between Employer's Liability Insurance and Work Injury Insurance demonstrates a nuanced coverage landscape. While Work Injury Insurance covers considerable work injury benefits, it still requires the employer to manage certain benefits like wages during work stoppage periods and one-time disability subsidies [Data: Entities (50); Relationships (39)]. Employer's Liability Insurance steps in to cover additional liabilities imposed by national laws, illustrating a layered protection strategy where multiple policies interlock to provide comprehensive coverage [Data: Relationships (39)].\n\n## Role of Group Accidental Injury Insurance\n\nGroup Accidental Injury Insurance appears to be more limited when compared with Employer's Liability Insurance, specifically visible in aspects such as exclusion of occupational diseases and lost wages support [Data: Entities (51); Relationships (38)]. Its inclusion along with more comprehensive insurances suggests its role as a supplementary coverage option, benefiting groups by filling certain gaps left by other more comprehensive policies [Data: Relationships (38)].\n\n## Impact on Labor Disputes and Employee Benefits\n\nThe combination of these insurance policies significantly impacts labor relation dynamics. By transferring risk from employer to insurer, Employer's Liability Insurance highly contributes to dispute mitigation and enhancement of employee benefits [Data: Entities (49)]. This system dilutes potential negative impacts of work-related incidents on the continuity and stability of work environments while ensuring that employees receive due benefits without direct conflict [Data: Relationships (39)].", "# Wang and Worker: A Case Study of Occupational Injury\n\nThe community focuses on an electrician named Wang, who suffered a significant injury leading to a disability while on the job, categorized under the broader classification of 'Worker.' This highlights critical aspects of labor safety, insurance claims, and worker rights.\n\n## Wang's occupational injury and its implications\n\nWang, identified as an Electrician who now lives with a 7th level disability resulting from a workplace accident, acts as a focal point in understanding occupational hazards and worker protection measures. This incident underscores the serious risks inherent in some job roles, particularly in industrial settings. Moreover, the consequences of such accidents—extending from personal health impact to broader socio-economic implications—are profound, bringing attention to the need for improved safety regulations and disability accommodations at work [Data: Entities (53)].\n\n## Classification of Wang among Workers\n\nWang's classification under the general entity of 'Worker' links him to a wider demographic that includes various forms of employment statuses such as permanent, temporary, and apprentices. This relationship illustrates the scope of labor rights and policies that are meant to protect workers but might often fail in preventing such accidents or in providing sufficient post-accident care. The connection between Wang and the general worker entity via labor and injury claims highlights systemic issues in worker safety and insurance protocols that need addressing to safeguard worker interests [Data: Entities (56), Relationships (44)].\n\n## Insights into labor insurance claims\n\nThe case of Wang as a worker who can claim insurance due to his injury sheds light on the legal and insurance framework surrounding workplace accidents. It points to the necessity for accessible, fair, and prompt compensation mechanisms for workers who suffer injuries. The efficacy of these systems directly impacts the livelihoods and recovery of injured workers, indicating a significant area of concern for labor rights advocacy and legal reforms. The specifics of the claim process, while not detailed, are crucial for understanding the adequacy of support provided to individuals in Wang's situation [Data: Relationships (44)]."], 'kwargs': {}}
10:53:27,938 graphrag.index.run.run_pipeline ERROR error running workflow generate_text_embeddings
Traceback (most recent call last):
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 143, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 49, in run_workflow
    output = await generate_text_embeddings(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 136, in generate_text_embeddings
    outputs[field] = await _run_and_snapshot_embeddings(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\workflows\generate_text_embeddings.py", line 155, in _run_and_snapshot_embeddings
    data["embedding"] = await embed_text(
                        ^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 89, in embed_text
    return await _text_embed_with_vector_store(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\embed_text\embed_text.py", line 183, in _text_embed_with_vector_store
    result = await strategy_exec(texts, callbacks, cache, strategy_config)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 68, in run
    embeddings = await _execute(model, text_batches, ticker, semaphore)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 97, in _execute
    results = await asyncio.gather(*futures)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\embed_text\strategies\openai.py", line 91, in embed
    chunk_embeddings = await model.aembed_batch(chunk)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\language_model\providers\fnllm\models.py", line 183, in aembed_batch
    response = await self.model(text_list, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\fnllm\base\base_llm.py", line 144, in __call__
    return await self._decorated_target(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\fnllm\base\services\cached.py", line 137, in invoke
    result = await delegate(prompt, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\fnllm\base\services\rate_limiter.py", line 75, in invoke
    result = await delegate(prompt, **args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\fnllm\base\base_llm.py", line 126, in _decorator_target
    output = await self._execute_llm(prompt, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\fnllm\openai\llm\openai_embeddings_llm.py", line 126, in _execute_llm
    result = await self._client.embeddings.create(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\resources\embeddings.py", line 243, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\_base_client.py", line 1564, in _request
    return await self._process_response(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\_base_client.py", line 1661, in _process_response
    return await api_response.parse()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\_response.py", line 432, in parse
    parsed = self._options.post_parser(parsed)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\openai\resources\embeddings.py", line 229, in parser
    for embedding in obj.data:
TypeError: 'NoneType' object is not iterable
10:53:27,977 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
10:53:28,6 graphrag.cli.index ERROR Errors occurred during the pipeline run, see logs for more details.
11:53:57,775 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
11:53:58,362 httpx INFO HTTP Request: POST http://localhost:11434/chat/completions "HTTP/1.1 404 Not Found"
11:55:01,379 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
11:55:01,950 httpx INFO HTTP Request: POST http://localhost:11434/chat/completions "HTTP/1.1 404 Not Found"
12:10:03,54 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
12:10:03,634 httpx INFO HTTP Request: POST http://localhost:11434/v1/v1/chat/completions "HTTP/1.1 404 Not Found"
12:11:41,138 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
12:11:41,715 httpx INFO HTTP Request: POST http://localhost:11434/v1/v1/chat/completions "HTTP/1.1 404 Not Found"
12:12:37,957 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
12:12:41,383 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:12:42,958 httpx INFO HTTP Request: POST http://localhost:11434/api/embeddings "HTTP/1.1 200 OK"
12:14:03,16 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
12:14:11,147 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:14:12,461 httpx INFO HTTP Request: POST http://localhost:11434/api/embeddings "HTTP/1.1 200 OK"
12:16:13,387 graphrag.cli.index INFO Logging enabled at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\logs\indexing-engine.log
12:16:21,492 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:16:23,10 httpx INFO HTTP Request: POST http://localhost:11434/v1/embeddings "HTTP/1.1 200 OK"
12:16:23,18 graphrag.cli.index INFO Starting pipeline run. dry_run=False
12:16:23,19 graphrag.cli.index INFO Using default configuration: {
    "root_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl",
    "models": {
        "default_chat_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_chat",
            "model": "deepseek-r1:7b",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        },
        "default_embedding_model": {
            "api_key": "==== REDACTED ====",
            "auth_type": "api_key",
            "type": "openai_embedding",
            "model": "quentinz/bge-large-zh-v1.5:latest",
            "encoding_model": "cl100k_base",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": "http://localhost:11434/v1",
            "api_version": null,
            "deployment_name": null,
            "proxy": null,
            "audience": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "retry_strategy": "native",
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "concurrent_requests": 25,
            "responses": null,
            "async_mode": "threaded"
        }
    },
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\logs",
        "storage_account_blob_url": null
    },
    "output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "outputs": null,
    "update_index_output": {
        "type": "file",
        "base_dir": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\update_output",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null,
        "cosmosdb_account_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "text_column": "text",
        "title_column": null,
        "metadata": null
    },
    "embed_graph": {
        "enabled": false,
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "use_lcc": true
    },
    "embed_text": {
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "names": [],
        "strategy": null,
        "model_id": "default_embedding_model",
        "vector_store_id": "default_vector_store"
    },
    "chunks": {
        "size": 200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": "tokens",
        "encoding_model": "cl100k_base",
        "prepend_metadata": false,
        "chunk_size_includes_metadata": false
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false
    },
    "extract_graph": {
        "prompt": "prompts/extract_graph.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "extract_graph_nlp": {
        "normalize_edge_weights": true,
        "text_analyzer": {
            "extractor_type": "regex_english",
            "model_name": "en_core_web_md",
            "max_word_length": 15,
            "word_delimiter": " ",
            "include_named_entities": true,
            "exclude_nouns": null,
            "exclude_entity_tags": [
                "DATE"
            ],
            "exclude_pos_tags": [
                "DET",
                "PRON",
                "INTJ",
                "X"
            ],
            "noun_phrase_tags": [
                "PROPN",
                "NOUNS"
            ],
            "noun_phrase_grammars": {
                "PROPN,PROPN": "PROPN",
                "NOUN,NOUN": "NOUNS",
                "NOUNS,NOUN": "NOUNS",
                "ADJ,ADJ": "ADJ",
                "ADJ,NOUN": "NOUNS"
            }
        },
        "concurrent_requests": 25
    },
    "summarize_descriptions": {
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "community_reports": {
        "graph_prompt": "prompts/community_report_graph.txt",
        "text_prompt": "prompts/community_report_text.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null,
        "model_id": "default_chat_model"
    },
    "extract_claims": {
        "enabled": false,
        "prompt": "prompts/extract_claims.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null,
        "model_id": "default_chat_model"
    },
    "prune_graph": {
        "min_node_freq": 2,
        "max_node_freq_std": null,
        "min_node_degree": 1,
        "max_node_degree_std": null,
        "min_edge_weight_pct": 40,
        "remove_ego_nodes": false,
        "lcc_only": false
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "use_lcc": true,
        "seed": 3735928559
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "community_prop": 0.15,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "reduce_prompt": "prompts/drift_search_reduce_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "reduce_max_tokens": 2000,
        "reduce_temperature": 0,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0,
        "local_search_top_p": 1,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 4096
    },
    "basic_search": {
        "prompt": "prompts/basic_search_system_prompt.txt",
        "chat_model_id": "default_chat_model",
        "embedding_model_id": "default_embedding_model",
        "text_unit_prop": 0.5,
        "conversation_history_max_turns": 5,
        "temperature": 0,
        "top_p": 1,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "vector_store": {
        "default_vector_store": {
            "type": "lancedb",
            "db_uri": "C:\\Users\\zhang\\MySpace\\dev\\coding\\python\\agi_study\\rag\\graphrag\\openl\\output\\lancedb",
            "url": null,
            "audience": null,
            "container_name": "==== REDACTED ====",
            "database_name": null,
            "overwrite": true
        }
    },
    "workflows": null
}
12:16:23,22 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\output
12:16:23,23 graphrag.index.input.factory INFO loading input from root_dir=input
12:16:23,23 graphrag.index.input.factory INFO using file storage for input
12:16:23,26 graphrag.storage.file_pipeline_storage INFO search C:\Users\zhang\MySpace\dev\coding\python\agi_study\rag\graphrag\openl\input for files matching .*\.txt$
12:16:23,42 graphrag.index.input.util INFO Found 7 InputFileType.text files, loading 7
12:16:23,42 graphrag.index.input.util INFO Total number of unfiltered InputFileType.text rows: 7
12:16:23,45 graphrag.index.run.run_pipeline INFO Final # of rows loaded: 7
12:16:23,59 graphrag.utils.storage INFO reading table from storage: documents.parquet
12:16:23,133 graphrag.utils.storage INFO reading table from storage: documents.parquet
12:16:23,140 graphrag.utils.storage INFO reading table from storage: text_units.parquet
12:16:23,191 graphrag.utils.storage INFO reading table from storage: text_units.parquet
12:16:42,426 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:02,127 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:09,361 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:15,43 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:21,102 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:33,108 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:39,697 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:45,13 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:17:51,661 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:04,342 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:09,915 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:12,119 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:18,885 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:26,901 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:28,794 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:41,130 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:47,440 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:18:52,421 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:07,52 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:15,178 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:20,36 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:23,744 openai._base_client INFO Retrying request to /chat/completions in 0.454792 seconds
12:19:23,745 openai._base_client INFO Retrying request to /chat/completions in 0.418468 seconds
12:19:23,745 openai._base_client INFO Retrying request to /chat/completions in 0.436953 seconds
12:19:23,745 openai._base_client INFO Retrying request to /chat/completions in 0.488388 seconds
12:19:30,292 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:37,443 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:41,182 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:48,204 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:50,479 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:51,679 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:53,354 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:19:59,669 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:05,449 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:09,278 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:15,398 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:20,405 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:23,771 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:27,815 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:36,603 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:39,506 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:41,172 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:54,63 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:20:56,271 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:07,454 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:11,186 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:13,663 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:23,981 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:31,931 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:44,546 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:46,307 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:53,368 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:21:58,456 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:07,428 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:14,991 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:17,76 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:18,438 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:26,158 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:32,278 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:37,584 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:51,174 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:51,982 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:57,583 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:22:59,933 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:12,646 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:19,996 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:22,276 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:24,869 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:35,701 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:40,113 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:54,404 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:23:55,501 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:02,223 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:11,986 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:13,633 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:17,91 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:31,696 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:34,563 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:35,10 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:35,960 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:44,958 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:51,910 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:52,986 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:53,560 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:24:58,89 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:04,374 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:06,795 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:08,57 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:11,257 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:17,956 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:20,369 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:23,665 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:33,90 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:44,709 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:51,494 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:25:57,798 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:00,745 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:05,6 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:34,124 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:36,370 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:37,887 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:51,928 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:26:57,958 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:02,565 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:04,975 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:22,308 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:24,680 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:30,449 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:45,750 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:56,332 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:27:58,96 openai._base_client INFO Retrying request to /chat/completions in 0.383155 seconds
12:28:03,525 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:28:08,81 openai._base_client INFO Retrying request to /chat/completions in 0.446259 seconds
12:28:11,276 openai._base_client INFO Retrying request to /chat/completions in 0.440477 seconds
12:28:17,977 openai._base_client INFO Retrying request to /chat/completions in 0.482315 seconds
12:28:20,389 openai._base_client INFO Retrying request to /chat/completions in 0.475058 seconds
12:28:23,689 openai._base_client INFO Retrying request to /chat/completions in 0.375167 seconds
12:28:33,109 openai._base_client INFO Retrying request to /chat/completions in 0.444525 seconds
12:28:44,725 openai._base_client INFO Retrying request to /chat/completions in 0.451499 seconds
12:28:50,727 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:28:51,516 openai._base_client INFO Retrying request to /chat/completions in 0.414887 seconds
12:29:00,765 openai._base_client INFO Retrying request to /chat/completions in 0.385003 seconds
12:29:05,23 openai._base_client INFO Retrying request to /chat/completions in 0.486023 seconds
12:29:06,796 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:07,440 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:23,149 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:25,991 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:29,681 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:34,160 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:45,228 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:49,76 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:51,624 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:29:55,19 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:30:03,215 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:30:04,935 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:30:18,740 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:30:35,855 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:30:44,885 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:30:46,593 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:07,792 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:08,815 openai._base_client INFO Retrying request to /chat/completions in 0.857633 seconds
12:31:20,937 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:21,128 openai._base_client INFO Retrying request to /chat/completions in 0.910403 seconds
12:31:25,607 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:36,966 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:38,977 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:50,638 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:31:52,214 openai._base_client INFO Retrying request to /chat/completions in 0.864869 seconds
12:32:02,153 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:32:07,475 openai._base_client INFO Retrying request to /chat/completions in 0.469875 seconds
12:32:20,843 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:32:22,472 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:32:29,694 openai._base_client INFO Retrying request to /chat/completions in 0.411029 seconds
12:32:34,189 openai._base_client INFO Retrying request to /chat/completions in 0.429829 seconds
12:32:45,253 openai._base_client INFO Retrying request to /chat/completions in 0.486427 seconds
12:32:49,116 openai._base_client INFO Retrying request to /chat/completions in 0.438495 seconds
12:32:51,635 openai._base_client INFO Retrying request to /chat/completions in 0.375473 seconds
12:32:55,51 openai._base_client INFO Retrying request to /chat/completions in 0.429254 seconds
12:33:03,237 openai._base_client INFO Retrying request to /chat/completions in 0.475456 seconds
12:33:04,955 openai._base_client INFO Retrying request to /chat/completions in 0.440306 seconds
12:33:11,462 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:18,766 openai._base_client INFO Retrying request to /chat/completions in 0.397705 seconds
12:33:20,522 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:27,724 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:34,633 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:41,930 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:48,127 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:52,25 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:33:56,41 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:00,224 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:10,273 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:14,836 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:22,316 openai._base_client INFO Retrying request to /chat/completions in 1.691633 seconds
12:34:30,11 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:32,127 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:39,764 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:34:43,968 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:02,687 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:08,294 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:09,129 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:17,92 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:27,23 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:35,631 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:35:45,953 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:01,772 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:05,265 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:05,676 openai._base_client INFO Retrying request to /chat/completions in 0.782784 seconds
12:36:20,547 openai._base_client INFO Retrying request to /chat/completions in 0.471270 seconds
12:36:25,423 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:27,624 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:27,754 openai._base_client INFO Retrying request to /chat/completions in 0.421885 seconds
12:36:35,670 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:52,51 openai._base_client INFO Retrying request to /chat/completions in 0.468421 seconds
12:36:55,25 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:36:56,51 openai._base_client INFO Retrying request to /chat/completions in 0.413611 seconds
12:37:10,299 openai._base_client INFO Retrying request to /chat/completions in 0.410588 seconds
12:37:11,495 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:19,183 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:24,285 openai._base_client INFO Retrying request to /chat/completions in 3.548227 seconds
12:37:26,846 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:30,29 openai._base_client INFO Retrying request to /chat/completions in 0.496625 seconds
12:37:42,246 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:43,444 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:45,487 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:51,323 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:37:57,322 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:02,658 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:06,485 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:08,130 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:14,11 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:27,446 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:30,129 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:39,978 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:41,659 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:38:47,406 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:04,486 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:07,121 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:25,666 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:26,114 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:40,11 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:49,545 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:52,212 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:39:55,585 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:40:08,13 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:40:26,446 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:40:27,266 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:40:28,109 openai._base_client INFO Retrying request to /chat/completions in 6.271791 seconds
12:40:30,187 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:40:45,528 openai._base_client INFO Retrying request to /chat/completions in 0.403417 seconds
12:40:51,344 openai._base_client INFO Retrying request to /chat/completions in 0.485993 seconds
12:40:53,728 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:41:02,684 openai._base_client INFO Retrying request to /chat/completions in 0.476261 seconds
12:41:06,509 openai._base_client INFO Retrying request to /chat/completions in 0.440342 seconds
12:41:08,153 openai._base_client INFO Retrying request to /chat/completions in 0.429787 seconds
12:41:14,26 openai._base_client INFO Retrying request to /chat/completions in 0.468481 seconds
12:41:27,471 openai._base_client INFO Retrying request to /chat/completions in 0.496729 seconds
12:41:30,154 openai._base_client INFO Retrying request to /chat/completions in 0.490462 seconds
12:41:34,19 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:41:41,676 openai._base_client INFO Retrying request to /chat/completions in 0.386174 seconds
12:41:46,471 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:41:53,447 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:41:53,961 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:00,56 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:02,81 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:10,990 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:15,109 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:17,701 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:18,528 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:32,817 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:42,728 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:49,25 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:52,88 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:42:58,524 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:06,371 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:08,602 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:30,119 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:35,637 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:37,996 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:45,311 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:49,585 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:51,142 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:43:52,442 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:05,739 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:07,379 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:24,559 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:26,974 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:27,6 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:42,257 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:43,277 httpx INFO HTTP Request: POST http://localhost:11434/v1/chat/completions "HTTP/1.1 200 OK"
12:44:43,302 graphrag.index.run.run_pipeline ERROR error running workflow extract_graph
Traceback (most recent call last):
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\run\run_pipeline.py", line 143, in _run_pipeline
    result = await workflow_function(config, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\workflows\extract_graph.py", line 46, in run_workflow
    entities, relationships = await extract_graph(
                              ^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\workflows\extract_graph.py", line 82, in extract_graph
    extracted_entities, extracted_relationships = await extractor(
                                                  ^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\extract_graph\extract_graph.py", line 132, in extract_graph
    entities = _merge_entities(entity_dfs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\graphrag\index\operations\extract_graph\extract_graph.py", line 156, in _merge_entities
    all_entities.groupby(["title", "type"], sort=False)
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\pandas\core\frame.py", line 9183, in groupby
    return DataFrameGroupBy(
           ^^^^^^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\pandas\core\groupby\groupby.py", line 1329, in __init__
    grouper, exclusions, obj = get_grouper(
                               ^^^^^^^^^^^^
  File "C:\Users\zhang\MySpace\dev\Anaconda3\Lib\site-packages\pandas\core\groupby\grouper.py", line 1043, in get_grouper
    raise KeyError(gpr)
KeyError: 'title'
12:44:43,392 graphrag.callbacks.file_workflow_callbacks INFO Error running pipeline! details=None
12:44:43,406 graphrag.cli.index ERROR Errors occurred during the pipeline run, see logs for more details.
